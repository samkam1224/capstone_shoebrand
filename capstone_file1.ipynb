{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d18064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from random import randint , choice\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "import sys,shutil, os,time ,multiprocessing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class customselenium():\n",
    "    __ver__ = '2.0' #Version of Custom Selenium \n",
    "    __dev__ = 'Sameer K' # Developed By \n",
    "\n",
    "\n",
    "    def __init__(self) -> None:      \n",
    "        self.wait = None\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        self.driver = None\n",
    "        self.file = self.__class__.__name__\n",
    "        self.random_user_agent = self.random_user() \n",
    "        self.chrome_ext = None\n",
    "        self.lock =  multiprocessing.Manager().Lock()\n",
    "        \n",
    "    def intialize_driver(self):\n",
    "        if self.driver is None:\n",
    "            self.driver = self.browser()\n",
    "            self.wait = WebDriverWait(self.driver,30)\n",
    "\n",
    "    def close(self):\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            self.driver = None\n",
    "            self.wait = None\n",
    "    \n",
    "    def readFile(self,file):\n",
    "        file,ext = file.split('.')\n",
    "        data_frame = pd.DataFrame()\n",
    "        try:\n",
    "            if ext == 'csv':\n",
    "                data_frame = pd.read_csv(f\"{file}.{ext}\")\n",
    "            elif ext == 'xlsx':\n",
    "                data_frame = pd.read_excel(f\"{file}.{ext}\")\n",
    "        except Exception as ex:\n",
    "            pass\n",
    "\n",
    "        return data_frame         \n",
    "   \n",
    "    def browser(self,head=False):                  \n",
    "        options = uc.ChromeOptions()\n",
    "        \n",
    "        # options.add_argument(f\"--headers={headers}\")\n",
    "        options.add_argument(\"--disable-popup-blocking\")\n",
    "        # options.add_argument(\"--force-device-scale-factor=0.8\")\n",
    "\n",
    "        # options.add_experimental_option('prefs' ,{\n",
    "        #     \"download.default_directory\": os.path.join(os.getcwd() , \"CaptchaImage\")\n",
    "        # })\n",
    "\n",
    "        # if not os.path.exists(os.path.join(os.getcwd() , \"CaptchaImage\")):\n",
    "        #     os.mkdir(os.path.join(os.getcwd() , \"CaptchaImage\"))\n",
    "\n",
    "        if self.chrome_ext is not None:\n",
    "            options.add_argument(f'--load-extension={self.chrome_ext}')  \n",
    "\n",
    "        # browser_executable_path = \".\\Google\\Chrome\\Application\\chrome.exe\"\n",
    "\n",
    "        # if not os.path.exists(browser_executable_path):\n",
    "        #     print('Chrome Folder Missing, make sure chrome folder exist in current working directory.')\n",
    "        #     input()\n",
    "        #     sys.exit(0)\n",
    "            \n",
    "\n",
    "        try:\n",
    "            driver = uc.Chrome(\n",
    "                options=options,\n",
    "                use_subprocess=True,\n",
    "                headless=head,\n",
    "                # browser_executable_path=browser_executable_path\n",
    "            )            \n",
    "        except Exception as ex:            \n",
    "            print('Chrome Intializing Error...probably Version error \\n') \n",
    "            input()          \n",
    "            sys.exit(0)   \n",
    "\n",
    "        driver.maximize_window()\n",
    "        print('Chrome Intialized...')\n",
    "        return driver\n",
    "            \n",
    "    def get_Xpath_value(self,xpath_Id,all_elements=False,wait_time = 15):        \n",
    "        self.wait = WebDriverWait(self.driver,wait_time)\n",
    "        if all_elements:\n",
    "            locate_element = EC.presence_of_all_elements_located\n",
    "        else:\n",
    "            locate_element = EC.presence_of_element_located        \n",
    "        try:\n",
    "            value = self.wait.until(locate_element((By.XPATH, xpath_Id)))\n",
    "        except Exception as ex:\n",
    "            value = None   \n",
    "        return value    \n",
    "    \n",
    "    def datime(self):\n",
    "        return datetime.today().timestamp()   \n",
    "    \n",
    "    def chunk_list(self, data, num_chunks):\n",
    "        chunk_size, remainder = divmod(len(data), num_chunks)\n",
    "        chunks = []\n",
    "        start = 0\n",
    "\n",
    "        for i in range(num_chunks):\n",
    "            if i < remainder:\n",
    "                end = start + chunk_size + 1\n",
    "            else:\n",
    "                end = start + chunk_size\n",
    "\n",
    "            chunks.append(data[start:end])\n",
    "            start = end\n",
    "\n",
    "        return chunks\n",
    "    \n",
    "    def remove_file(self):\n",
    "        try:\n",
    "            os.remove(f\"{self.file}\") if \"xlsx\" in self.file else os.remove(f\"{self.file}.xlsx\")    \n",
    "        except Exception as ex:\n",
    "            pass\n",
    "    \n",
    "    def randomSleep(self,start,end):        \n",
    "        time.sleep(randint(start,end))\n",
    "\n",
    "    def openNewWindow(self):\n",
    "        try:\n",
    "            self.driver.execute_script(\"window.open(arguments[0], '_blank')\")\n",
    "            self.driver.switch_to.window(self.driver.window_handles[-1])\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            print(\"Driver is not available. Make sure the driver is started.\")\n",
    "        return self.driver\n",
    "    \n",
    "    def save(self,name = None,trig=False,**kwargs):\n",
    "        if name:\n",
    "            self.file = name  \n",
    "\n",
    "        try:\n",
    "            dataframe = pd.read_excel(f\"{self.file}.xlsx\")\n",
    "        except FileNotFoundError:\n",
    "            dataframe = pd.DataFrame()        \n",
    "        \n",
    "        if trig:\n",
    "            dataframe1 = pd.DataFrame(kwargs)\n",
    "        else:\n",
    "            dataframe1 = pd.DataFrame([kwargs])\n",
    "\n",
    "        dataframe = pd.concat([dataframe,dataframe1],ignore_index=True)\n",
    "        dataframe.to_excel(f\"{self.file}.xlsx\",index=False)\n",
    "\n",
    "    def click(self,xpath,wait_time=15):\n",
    "        self.wait = WebDriverWait(self.driver,wait_time)\n",
    "        try:\n",
    "            element = self.wait.until(EC.presence_of_element_located((By.XPATH,xpath)))\n",
    "            self.scrollToElement(element=element)\n",
    "            self.randomSleep(2,3)\n",
    "            element.click()\n",
    "            return True\n",
    "        except Exception as ex:\n",
    "            return False\n",
    "            \n",
    "    def get_page_source(self,element=None):\n",
    "        try:\n",
    "            if element:\n",
    "                source = element.get_attribute(\"innerHTML\")\n",
    "            else:\n",
    "                source = self.driver.page_source              \n",
    "            soup = BeautifulSoup(source,'lxml')            \n",
    "        except Exception as ex : \n",
    "            soup = None\n",
    "        \n",
    "        return soup\n",
    "\n",
    "    def scrollToElement(self,element):     \n",
    "\n",
    "            \n",
    "        try:\n",
    "            self.driver.execute_script(\n",
    "                    \"arguments[0].scrollIntoView({behavior: 'auto', block: 'center', inline: 'center'});\",\n",
    "                    element,\n",
    "            )\n",
    "            \n",
    "\n",
    "            \n",
    "            x = element.location.get('x')\n",
    "            y = element.location.get('y')\n",
    "            \n",
    "            if (self.x == x) and (self.y == y) :\n",
    "                return False\n",
    "        \n",
    "            self.x = x\n",
    "            self.y = y   \n",
    "            \n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "        \n",
    "        \n",
    "    \n",
    "    def random_user(self):\n",
    "        browsers = (\n",
    "            \"Mozilla\",\n",
    "            \"Opera\",\n",
    "            \"Internet Explorer\",\n",
    "            \"Chrome\",\n",
    "            \"Safari\",\n",
    "        )\n",
    "\n",
    "        operating_system = (\n",
    "            \"Windows NT 10.0; Win64; x64\",\n",
    "            \"Windows NT 6.1; WOW64\",\n",
    "            \"Macintosh; Intel Mac OS X 10_15_4\",\n",
    "            \"X11; Linux x86_64\",\n",
    "            \"Android 10.0; Mobile\",\n",
    "            \"iOS 13.4; iPhone\",\n",
    "        )\n",
    "\n",
    "        version = {\n",
    "            \"Mozilla\": f\"{randint(3, 15)}.0\",\n",
    "            \"Opera\": f\"{randint(3, 15)}.80\",\n",
    "            \"Internet Explorer\": f\"{randint(3, 15)}.0\",\n",
    "            \"Chrome\": f\"{randint(70, 110)}.0.3987.{randint(100, 999)}\",\n",
    "            \"Safari\": f\"{randint(500, 550)}.36\",\n",
    "        }\n",
    "\n",
    "        browser = choice(browsers)\n",
    "        os = choice(operating_system)\n",
    "        browser_version = version[browser]\n",
    "\n",
    "        user_agent = f\"{browser}/{browser_version} ({os})\"\n",
    "\n",
    "        return user_agent\n",
    "    \n",
    "    def validateData(self,element):\n",
    "        if element :\n",
    "            return element.text\n",
    "        return None\n",
    "        \n",
    "    def load_extension(self,name,version):\n",
    "        self.chrome_ext = os.path.join(os.getcwd(),name,version)\n",
    "    \n",
    "    def close_extension_tab(self):\n",
    "        try:\n",
    "            self.get_Xpath_value(xpath_Id='//p[@class=\"installed-loading__lead-paragraph\"]')\n",
    "            self.driver.switch_to.window(self.driver.window_handles[-1])  \n",
    "            self.driver.close()\n",
    "            self.driver.switch_to.window(self.driver.window_handles[-1])\n",
    "        except:\n",
    "            print('Disk Full ....Free Up Some Space')\n",
    "            sys.exit(0)\n",
    "    \n",
    "    # Multi Processing Execute Function\n",
    "            \n",
    "    def stop_exc(self,mesg=None):\n",
    "        print(mesg)\n",
    "        input()\n",
    "        self.close()\n",
    "        sys.exit(0)\n",
    "    \n",
    "    def execute(self):        \n",
    "        try:\n",
    "            links = pd.read_excel(f\"{self.file}_link.xlsx\")['Links']\n",
    "            self.file = f\"{self.file}_link.xlsx\"\n",
    "            print(\"\\nExtraction Resumes...\")\n",
    "        except:\n",
    "            links = pd.DataFrame()\n",
    "            self.scrape_links()\n",
    "\n",
    "        self.driver.close()        \n",
    "        \n",
    "        if links.size == 0:\n",
    "            links = pd.read_excel(f\"{self.file}.xlsx\")['Links']\n",
    "\n",
    "        if links.size > 10 and links.size < 20:\n",
    "            num_processes = 2\n",
    "        elif links.size <= 5:\n",
    "            num_processes = 1\n",
    "        else:\n",
    "            num_processes = int(multiprocessing.cpu_count()) // 2\n",
    "            num_processes += 2\n",
    "        \n",
    "        num_processes = 1\n",
    "        \n",
    "        if links.size == 0:\n",
    "            print(\"No Links Found To Extract Data.....\")\n",
    "            return        \n",
    "\n",
    "        url_chunks = self.chunk_list(links,num_processes)        \n",
    "        instance = self.__class__()\n",
    "\n",
    "        print(\"Relax Now ...Extracting Data From the Links.\\n\")\n",
    "\n",
    "        with multiprocessing.Pool(processes=num_processes) as pool:            \n",
    "            pool.starmap(instance.scrape_data, [(instance, url_chunk) for url_chunk in url_chunks])\n",
    "\n",
    "        print(\"Extraction Completed\\n\")\n",
    "        self.remove_file()\n",
    "        self.clean_temp()\n",
    "        # self.collateTemfiles()\n",
    "\n",
    "    def clean_temp(self):\n",
    "        temp_path = os.environ.get('TMP')\n",
    "        temp_files  = os.listdir(temp_path)\n",
    "        if temp_files:\n",
    "            for file in temp_files:\n",
    "                try:                  \n",
    "                    if os.path.isdir(os.path.join(temp_path,file)):\n",
    "                        shutil.rmtree(os.path.join(temp_path,file))\n",
    "                    else:\n",
    "                        os.remove(os.path.join(temp_path,file))\n",
    "                except Exception as ex:\n",
    "                    continue                   \n",
    "\n",
    "    def collateTemfiles(self):\n",
    "        list_of_xl_dataframe = list()\n",
    "        path = os.path.join(os.getcwd(),'Data')\n",
    "        list_dir = os.listdir(path)\n",
    "\n",
    "        for file in list_dir:\n",
    "            list_of_xl_dataframe.append(pd.read_excel(f\"{path}/{file}\"))\n",
    "        \n",
    "        col_xl = pd.concat(list_of_xl_dataframe,ignore_index=True)\n",
    "        col_xl.to_excel(f\"Data/{self.file}\",index=False)\n",
    "\n",
    "        shutil.rmtree(path=path)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70b0a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CustomSelenuim import customselenium\n",
    "import multiprocessing,threading\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Addidas(customselenium):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self.links = list()\n",
    "        self.file_lock = multiprocessing.Manager().Lock()\n",
    "        self.base_link = \"https://www.adidas.com\"\n",
    "        super().__init__()\n",
    "\n",
    "    \n",
    "    def closepopup(self,driver):\n",
    "        while True:\n",
    "            driver.click(xpath=\"//button[@name='account-portal-close']\",wait_time=1)\n",
    "    \n",
    "    def scrape_links(self):       \n",
    "        \n",
    "        soup = self.get_page_source()\n",
    "        links = soup.select('a.glass-product-card__assets-link')        \n",
    "        self.links += [f\"{self.base_link}{link.attrs.get('href')}\" for link in links if link and \"href\" in link.attrs]\n",
    "\n",
    "            \n",
    "        link = {\n",
    "            'Links' : self.links,\n",
    "        }\n",
    "\n",
    "        self.save(name=f\"{self.file}_link\", trig=True,**link)\n",
    "        print(\"Links Extractions Completed \\n\")\n",
    "\n",
    "    def getQualityScore(self,quality):\n",
    "        if quality and \"style\" in quality.attrs:\n",
    "               value = quality.attrs.get('style').split(';')[1].split(':')[-1]\n",
    "               if \"%\" in value:\n",
    "                   value = value.replace('%','')\n",
    "               return round(float(value))\n",
    "        \n",
    "    def scrape_data(self,instance,url_chunk):\n",
    "        instance.intialize_driver()\n",
    "        new_thread = threading.Thread(target=instance.closepopup,args=(instance,),name=\"popup_close\")\n",
    "        new_thread.start()\n",
    "\n",
    "        for link in url_chunk:\n",
    "            if not link:\n",
    "                continue\n",
    "            \n",
    "            data = {}\n",
    "            \n",
    "            instance.driver.get(url=link.replace('us/en/',''))            \n",
    "            instance.click(xpath=\"//div[@class='reviews-header___1kFSZ']\",wait_time=30)\n",
    "            instance.click(xpath=\"//div[@id='navigation-target-specifications']/div\",wait_time=30)\n",
    "            soup = instance.get_page_source()\n",
    "            category = soup.select_one('div[data-auto-id=\"product-category\"]')\n",
    "            title = soup.select_one('h1.name___120FN > span')\n",
    "            price = soup.select_one('div.product-price___2Mip5 > div > div > div > div')\n",
    "            rating = soup.select_one('div.ratings-label-container___13pr- > span')\n",
    "            code = soup.select_one('ul.gl-list:nth-of-type(2) li:last-of-type')\n",
    "            color = soup.select_one('ul.gl-list:nth-of-type(2) li:nth-last-of-type(2)')\n",
    "            sizes = soup.select('button[class=\"gl-label size___2lbev\"] span')\n",
    "            review = soup.select_one('h2.accordion-title___2sTgR')\n",
    "            cmf_quality = soup.select_one('div.sub-ratings___1pAhV > div:nth-of-type(1) > div > div:nth-of-type(2) > div:nth-of-type(1)')\n",
    "            quality = soup.select_one('div.sub-ratings___1pAhV > div:nth-of-type(2) > div > div:nth-of-type(2) > div:nth-of-type(1)')\n",
    "            size_quality = soup.select_one('div.sub-ratings___1pAhV > div:nth-of-type(3) > div > div:nth-of-type(2) > div:nth-of-type(1)')\n",
    "            width_quality = soup.select_one('div.sub-ratings___1pAhV > div:nth-of-type(4) > div > div:nth-of-type(2) > div:nth-of-type(1)')\n",
    "\n",
    "            cmf_quality = instance.getQualityScore(cmf_quality)\n",
    "            if  cmf_quality is not None:\n",
    "\n",
    "                if cmf_quality  > 50:\n",
    "                    cmf_quality = f\"Comfortable {cmf_quality}%\"\n",
    "                else:\n",
    "                    cmf_quality = f\"Uncomfortable {cmf_quality}%\"\n",
    "            else:\n",
    "                cmf_quality = \"--\"\n",
    "                \n",
    "\n",
    "            quality = instance.getQualityScore(quality)\n",
    "            if quality:\n",
    "                if quality > 50:\n",
    "                    quality = f\"Perfect {quality}% \"\n",
    "                else:\n",
    "                    quality = f\"Poor {quality}%\"\n",
    "            else:\n",
    "\n",
    "                cmf_quality = \"--\"\n",
    "\n",
    "            size_quality = instance.getQualityScore(size_quality)\n",
    "            if size_quality:\n",
    "                if size_quality < 34:\n",
    "                    size_quality = f\"Too small {size_quality}%\"\n",
    "                elif size_quality > 34 and size_quality < 68:\n",
    "                    size_quality = f\"Perfect {size_quality}%\"\n",
    "                else:\n",
    "                    size_quality = f\"Too large {size_quality}%\"\n",
    "            else:\n",
    "                size_quality = '--'\n",
    "\n",
    "            \n",
    "            width_quality = instance.getQualityScore(width_quality)\n",
    "            if width_quality:\n",
    "                if width_quality < 34:\n",
    "                    width_quality = f\"Too narrow {width_quality}%\"\n",
    "                elif width_quality > 34 and width_quality < 68:\n",
    "                    width_quality = f\"Perfect {width_quality}%\"\n",
    "                else:\n",
    "                    width_quality = f\"Too wide {width_quality}%\"\n",
    "            else:\n",
    "                width_quality = '--'\n",
    "\n",
    "            data = {\n",
    "                'Category' : category.text if category else None,\n",
    "                'Title': title.text if title else None,\n",
    "                'Price': price.text if price else None,\n",
    "                'Code': code.text if code else None,                \n",
    "                'Available Size': ' '.join([size.text for size in sizes if size]),\n",
    "                'Rating' : rating.text if rating else None,                \n",
    "                'No of Reviews' : review.text if review else None,\n",
    "                'No of Size':len(sizes) if sizes else 0,\n",
    "                'No of Color': len(color.next.split('/')) if color else 0, \n",
    "                'Comfort' : cmf_quality,\n",
    "                'Quality' : quality,\n",
    "                'Size' : size_quality,\n",
    "                'Width' : width_quality\n",
    "            }\n",
    "\n",
    "            with instance.file_lock:\n",
    "                self.save(**data)                \n",
    "\n",
    "        instance.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.adidas.com/us/women-shoes\"\n",
    "    add = Addidas()\n",
    "    add.intialize_driver()\n",
    "    add.driver.get(url=url)\n",
    "    add.execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de998f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CustomSelenuim import customselenium\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "class Puma(customselenium):\n",
    "    def __init__(self) -> None:\n",
    "        self.links = list()\n",
    "        self.base_link = \"https://us.puma.com/\"\n",
    "        self.file_lock = multiprocessing.Manager().Lock()\n",
    "        super().__init__()\n",
    "    \n",
    "    def scrape_links(self):        \n",
    "\n",
    "            \n",
    "        soup = self.get_page_source()\n",
    "        links = soup.select('li[data-test-id=\"product-list-item\"] a')\n",
    "        self.links += [f\"{self.base_link}{link.attrs.get('href')}\" for link in links if link and \"href\" in link.attrs]\n",
    "        \n",
    "        link = {\n",
    "            'Links' : self.links,\n",
    "        }\n",
    "        self.save(name=f\"{self.file}_link\", trig=True,**link)\n",
    "        print(\"Links Extractions Completed \\n\")\n",
    "    \n",
    "    def scrape_data(self,instance,url_chunk):\n",
    "        \n",
    "        instance.intialize_driver()\n",
    "        for link in url_chunk:\n",
    "            \n",
    "            if not link:\n",
    "                continue\n",
    "            \n",
    "            data = {}\n",
    "            \n",
    "            instance.driver.get(url=link.replace('us/en/',''))\n",
    "            instance.click(xpath='//button[@data-test-id=\"read-all-reviews\"]')\n",
    "            self.randomSleep(2,2)\n",
    "            \n",
    "            soup = instance.get_page_source()\n",
    "            category = soup.select_one('ul[data-uds-child=\"breadcrumb-list\"] > li:nth-of-type(2)')\n",
    "            title = soup.select_one('h1#pdp-product-title')\n",
    "            price = soup.select_one('div[data-test-id=\"pdp-price\"] span:nth-of-type(1)')\n",
    "            code = soup.select_one('ul.tw-1h4nwdw.tw-p9uz4a.tw-xwzea6.list-disc.list-inside li:nth-of-type(1)')\n",
    "            color = soup.select('div#style-picker label span')\n",
    "            sizes = soup.select('div#size-picker label span span:last-of-type')\n",
    "            review = soup.select_one('div#product-reviews > div:nth-of-type(1) > h2')\n",
    "            rating = soup.select_one('p[data-test-id=\"percentRecommendThisProduct\"]')\n",
    "            \n",
    "            if rating:\n",
    "                rating = rating.text.split('%')[0]                \n",
    "                rating = float(rating) / 10\n",
    "                rating = rating / 2\n",
    "            else:\n",
    "                rating = 0.0\n",
    "                \n",
    "\n",
    "            data = {\n",
    "                'Category': category.text if category else None,\n",
    "                'Title': title.text if title else None,\n",
    "                'Price': price.text if price else None,\n",
    "                'Code': code.text if code else None,\n",
    "                'Available Size': ' '.join([size.text for size in sizes if size]),     \n",
    "                'No of Size':len(sizes) if sizes else 1,\n",
    "                'No of Color': len(color) if color else 0,\n",
    "                'review' : review.text if review else None,\n",
    "                'rating': rating\n",
    "            \n",
    "            }\n",
    "            \n",
    "            with instance.file_lock:\n",
    "                self.save(**data)\n",
    "\n",
    "        instance.close()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    puma = Puma()\n",
    "    puma.intialize_driver()\n",
    "    puma.driver.get(url=\"https://us.puma.com/us/en/women/shoes\") # https://us.puma.com/us/en/men/shop-all-mens#\n",
    "    puma.execute()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16732c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CustomSelenuim import customselenium \n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "class Nike(customselenium):\n",
    "    def __init__(self) -> None:\n",
    "        self.links = list()\n",
    "        self.file_lock = multiprocessing.Manager().Lock()\n",
    "        super().__init__()\n",
    "\n",
    "    def scrape_links(self):        \n",
    "\n",
    "        soup = self.get_page_source()\n",
    "        \n",
    "        links = soup.select('a.product-card__link-overlay')\n",
    "        self.links += [link.attrs.get('href') for link in links if link and \"href\" in link.attrs]\n",
    "        \n",
    "        link = {\n",
    "            'Links' : self.links,\n",
    "        }\n",
    "\n",
    "        self.save(name=f\"{self.file}_link\", trig=True,**link)\n",
    "        print(\"Links Extractions Completed \\n\")\n",
    "\n",
    "    def scrape_data(self,instance,url_chunk):\n",
    "        \n",
    "        instance.intialize_driver()\n",
    "        for link in url_chunk:\n",
    "            if not link:\n",
    "                continue\n",
    "            \n",
    "            data = {}\n",
    "            \n",
    "            instance.driver.get(url=link)\n",
    "\n",
    "            instance.click(xpath='//details[@data-test=\"reviewsAccordionClick\"]')\n",
    "\n",
    "            soup = instance.get_page_source()\n",
    "            \n",
    "            title = soup.select_one('h1[data-test=\"product-title\"]')\n",
    "            price = soup.select_one('div[data-test=\"product-price\"]')\n",
    "            category = soup.select_one('h2[data-test=\"product-sub-title\"]')\n",
    "            code = soup.select_one('li.description-preview__style-color')\n",
    "            color = soup.select('div.colorway-images-wrapper fieldset div div label img')\n",
    "            \n",
    "            if not color:\n",
    "                color = soup.select_one('li.description-preview__color-description.ncss-li')\n",
    "                color = len(color.next.split('/')) if color else 0\n",
    "            else:\n",
    "                color = len(color)\n",
    "\n",
    "\n",
    "            sizes = soup.select('label.css-xf3ahq')\n",
    "            review = soup.select_one('details[data-test=\"reviewsAccordionClick\"] summary h3')\n",
    "            rating = soup.select_one('details[data-test=\"reviewsAccordionClick\"] summary div')\n",
    "\n",
    "            data = {\n",
    "                'Category': category.text if category else None,\n",
    "                'Title': title.text if title else None,\n",
    "                'Price': price.text if price else None,\n",
    "                'Code': code.text if code else None,             \n",
    "                'Available Size': ' '.join([size.text for size in sizes if size]),\n",
    "                'No of Size':len(sizes) if sizes else 1,\n",
    "                'No of Color': color, \n",
    "                'review' : review.text if review else None,\n",
    "                'rating':rating.attrs.get('aria-label') if rating else 0.0\n",
    "            }\n",
    "\n",
    "            with instance.file_lock:\n",
    "                self.save(**data)\n",
    "\n",
    "        instance.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Just Pass the url of category it will extract the data\n",
    "    # default url is passed men's shoes\n",
    "    \n",
    "    url = \"https://www.nike.com/in/w/womens-shoes-5e1x6zy7ok\" \n",
    "    nik = Nike()\n",
    "    nik.intialize_driver()\n",
    "    nik.driver.get(url=url)\n",
    "    nik.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44478a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CustomSelenuim import customselenium \n",
    "import multiprocessing\n",
    "\n",
    "class Skeechers(customselenium):\n",
    "    def __init__(self) -> None:\n",
    "        self.links = list()\n",
    "        self.base_url = \"https://www.skechers.com\"\n",
    "        self.file_lock = multiprocessing.Manager().Lock()\n",
    "        super().__init__()\n",
    "\n",
    "    def scrape_links(self):        \n",
    "               \n",
    "        soup = self.get_page_source()\n",
    "        links = soup.select('div.product > div > div > a')  \n",
    "        self.links += [f\"{self.base_url}{link.attrs.get('href')}\"for link in links if link and \"href\" in link.attrs]\n",
    "                \n",
    "        link = {\n",
    "            'Links' : self.links,\n",
    "        }\n",
    "\n",
    "        self.save(name=f\"{self.file}_link\", trig=True,**link)\n",
    "        print(\"Links Extractions Completed \\n\")\n",
    "\n",
    "    def scrape_data(self,instance,url_chunk):\n",
    "        instance.intialize_driver()\n",
    "        \n",
    "        for link in url_chunk:\n",
    "\n",
    "            instance.randomSleep(2,3)\n",
    "            \n",
    "            if not link:\n",
    "                continue\n",
    "            \n",
    "            data = {}\n",
    "            \n",
    "            instance.driver.get(url=link)\n",
    "            soup = instance.get_page_source()\n",
    "            category = soup.select_one('div.c-product-details__label')            \n",
    "            title = soup.select_one('h1.c-product-details__product-name.product-name')\n",
    "            price = soup.select_one('div.price > span > span > span > span')\n",
    "            colors = soup.select('button[aria-describedby=\"color\"]')\n",
    "            sizes = soup.select('span.size-value.c-product-attributes__item__value--size.c-product-attributes__item__value.swatch-value.selectable')\n",
    "            rating = soup.select_one('div.ratings > span')\n",
    "            review  = soup.select_one('div.pr-snippet-read-and-write a')\n",
    "            code = soup.select_one('span.product-id')\n",
    "\n",
    "            data = {\n",
    "                'Category': category.text if category else None,\n",
    "                'Title': title.text if title else None,\n",
    "                'Price': price.text if price else None,         \n",
    "                'Code': code.text if code else None,\n",
    "                'Available Size': ' '.join([size.text for size in sizes if size]),     \n",
    "                'No of Size':len(sizes) if sizes else 1,                  \n",
    "                'No of Color': len(colors) if colors else 1,\n",
    "                'review' : review.text if review else \"0 Reviews\",\n",
    "                'Rating' :rating.text if rating else None,\n",
    "            }\n",
    "\n",
    "            with instance.file_lock:\n",
    "                self.save(**data)\n",
    "\n",
    "        instance.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Just Pass the url of category it will extract the data\n",
    "    # default url is passed men's shoes\n",
    "    \n",
    "    url = \"https://www.skechers.com/women/shoes/\" \n",
    "    sk = Skeechers()\n",
    "    sk.intialize_driver()\n",
    "    sk.driver.get(url=url)\n",
    "    sk.execute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
